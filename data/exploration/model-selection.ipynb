{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763fab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "QUERY = (\n",
    "    'SELECT * FROM `article-source.article_views.hackernews`')\n",
    "query_job = client.query(QUERY)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470753ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = query_job.result()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6103b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryJob<project=article-source, location=US, id=e006ba67-00dd-45a7-940c-5a814a34fc67>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e563eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephpeart/Documents/python-projects/pb/data/exploration/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee1070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d0b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = next(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be046bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I think these sites look very attractive, but this is a very subjective and specific definition of \"good\". Many of these pages do not finish loading in 3 seconds, for example. I\\'m not sure it would be correct to frame these as \"astronomically good\" web design.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b221a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.ycombinator.com/item?id=37226805'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row.detail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d200c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = first_row.comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ac1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_tokens = tokenizer(comment, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b688b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   100,   206,   209,  3091,   356,   182,  6043,     6,    53,\n",
       "            42,    16,    10,   182, 22262,     8,  2167,  8515,     9,    22,\n",
       "          8396,   845,  1876,     9,   209,  6052,   109,    45,  2073, 16761,\n",
       "            11,   155,  2397,     6,    13,  1246,     4,    38,   437,    45,\n",
       "           686,    24,    74,    28,  4577,     7,  5120,   209,    25,    22,\n",
       "          1988,  2839,  1075,  3435,   205,   113,  3748,  1521,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "32126a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aea4b586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5851, -6.7813, -5.6964, -4.2389, -1.5744, -5.4951,  0.5477, -4.2243,\n",
       "         -6.1095, -3.7572, -1.6259, -5.5178, -6.2968, -5.6325, -5.9487, -4.7712,\n",
       "         -7.2741, -5.5293, -3.9624, -6.3900, -4.0808, -6.0886, -3.5761, -6.4716,\n",
       "         -6.1779, -6.1516, -5.6621, -2.7154]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ec08626-0c84-4d0e-9305-e60ba1a17134",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpipe = pipe(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae9ad7ef-63cf-4dac-9c4b-dad56a66a678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'confusion', 'score': 0.633594274520874}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0d9c1f7-7d6b-42de-81cf-802000f1f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3a8bdf4-83d2-4f0d-9043-abd05f984976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = torch.nn.Softmax(dim=0)\n",
    "np.argmax(sm(output.logits[0]).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f086e34-7c75-40b3-a1f0-c271dd3b5f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59183"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm(output.logits[0]).detach().numpy()[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a911c01a-5241-4a3e-91cb-3b5517a95a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think these sites look very attractive, but this is a very subjective and specific definition of \"good\". Many of these pages do not finish loading in 3 seconds, for example. I\\'m not sure it would be correct to frame these as \"astronomically good\" web design.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e82ae64-99fb-43f8-9af3-016deb439421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af254a49-7db0-4eac-926a-fe66b3b114eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c21c20b3-80ef-40c3-a765-3961c8e2dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c8420d3-81d9-4c1b-814b-e9d5c715484c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3f709e63-719e-44fa-98b7-511a3711449a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5388, 0.4612])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0d2ea5a-502e-4469-8023-eb41acefb2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b895fa7-4282-41f4-b300-d56b8d8cf269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.ycombinator.com/item?id=37226805'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row.detail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d005c7bb-028e-4f01-838b-2dea3a2090b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5268, 0.4732])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef02b75c-860d-424b-add3-1d933b2e3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = query_job.result()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88e1a797-0aeb-4365-8e21-1cce88e2217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "scored_rows = []\n",
    "\n",
    "# Test: average of the classification scores of the comments. Score of 1 = all positive. \n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    total = 0\n",
    "    for comment in row.comments:\n",
    "        inputs = tokenizer(comment, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            total += logits.argmax().item()\n",
    "    row_dict['score'] = (len(row.comments) - total) / len(row.comments)\n",
    "    scored_rows.append(row_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "902423b5-19a6-4496-80c3-9771fd7a3029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scored_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b098e953-cdc4-4dd2-bb3a-c09c1b7523f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rows = sorted(scored_rows, key=lambda row: row['score'], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9c6bf8a7-efae-4d8b-af2a-31f66e7bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in sorted_rows:\n",
    "    print(row['url'])\n",
    "    print(row['score'])\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Output removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6fcb55f8-494d-4c5e-8182-2e773920e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = query_job.result()  \n",
    "\n",
    "scored_rows = []\n",
    "\n",
    "# Test: average ratio of pos:neg odds (non-normalized) \n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    total = 0\n",
    "    for comment in row.comments:\n",
    "        inputs = tokenizer(comment, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            total += logits[0,0] / logits[0,1] \n",
    "\n",
    "\n",
    "    \n",
    "    row_dict['score'] = total / len(row.comments)\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4378d10a-287b-4474-aac8-23af2b3a61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rows_2 = sorted(scored_rows, key=lambda row: row['score'], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "cc6eee17-5ec6-4812-a182-914877450fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in sorted_rows_2:\n",
    "    print(row['url'])\n",
    "    print(row['score'])\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Output removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ee6689c5-e1d1-42fa-bb2f-9f9270a2135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Softmax(dim=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52e3dcbe-7812-4c65-82ec-876c28582d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = query_job.result() \n",
    "sm = torch.nn.Softmax(dim=1)\n",
    "scored_rows = []\n",
    "\n",
    "# Test: average ratio of pos:neg odds (normalized) \n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    total = 0\n",
    "    for comment in row.comments:\n",
    "        inputs = tokenizer(comment, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = sm(model(**inputs).logits)\n",
    "            total += logits[0,0] / logits[0,1] \n",
    "\n",
    "            # print(logits[0,0] / logits[0,1])\n",
    "            # print(total)\n",
    "\n",
    "    \n",
    "    # print(total / len(row.comments))\n",
    "    row_dict['score'] = total / len(row.comments)\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0443c605-3d8e-4d7e-bfdb-73cf714c18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rows_3 = sorted(scored_rows, key=lambda row: row['score'], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "5b11ac70-3c00-411e-90ad-29fc2843d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in sorted_rows_3:\n",
    "    print(row['url'])\n",
    "    print(row['score'])\n",
    "    print('\\n\\n')\n",
    "# Output removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9d29b-4142-452c-86a9-63b0602272e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c96825ff-31d9-43c7-bb78-a8d54dc30dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2e9bfff4-ca10-40d8-af98-053c3e1cdb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(query_job.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "21ff9b3f-f159-48b6-bb6c-5dd2712b368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_tokens = tokenizer(rows[0]['comments'][0], return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    output = model(**comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4ca7bd3f-3e3e-416e-a074-a2e3a17cd9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'admiration',\n",
       " 1: 'amusement',\n",
       " 2: 'anger',\n",
       " 3: 'annoyance',\n",
       " 4: 'approval',\n",
       " 5: 'caring',\n",
       " 6: 'confusion',\n",
       " 7: 'curiosity',\n",
       " 8: 'desire',\n",
       " 9: 'disappointment',\n",
       " 10: 'disapproval',\n",
       " 11: 'disgust',\n",
       " 12: 'embarrassment',\n",
       " 13: 'excitement',\n",
       " 14: 'fear',\n",
       " 15: 'gratitude',\n",
       " 16: 'grief',\n",
       " 17: 'joy',\n",
       " 18: 'love',\n",
       " 19: 'nervousness',\n",
       " 20: 'optimism',\n",
       " 21: 'pride',\n",
       " 22: 'realization',\n",
       " 23: 'relief',\n",
       " 24: 'remorse',\n",
       " 25: 'sadness',\n",
       " 26: 'surprise',\n",
       " 27: 'neutral'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fee66bc0-62c7-4d1e-af0a-62419c5bf812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.2243)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_emotion = 7\n",
    "output.logits[0][target_emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cd728abd-939a-47e6-8e1e-559ecf77144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = query_job.result() \n",
    "target_emotion = 7\n",
    "scored_rows = []\n",
    "\n",
    "# Test: roberta emotions - rank by average total curiosity \n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    total = 0\n",
    "    for comment in row.comments:\n",
    "        inputs = tokenizer(comment, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            total += np.exp(logits[0][target_emotion])\n",
    "\n",
    "            # print(logits[0,0] / logits[0,1])\n",
    "            # print(total)\n",
    "\n",
    "    \n",
    "    # print(total / len(row.comments))\n",
    "    row_dict['score'] = total / len(row.comments)\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b929d40-f326-4509-884b-5a9f3fa5db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rows_4 = sorted(scored_rows, key=lambda row: row['score'], reverse=True)\n",
    "\n",
    "for row in sorted_rows_4:\n",
    "    print(row['url'])\n",
    "    print(row['score'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036172f-4793-4925-9ff9-824f7adfff82",
   "metadata": {},
   "source": [
    "\n",
    "This is pretty cool. I'll go with this for now.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "939febb3-1f04-4b02-96d5-0e829f043909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = query_job.result() \n",
    "target_emotion = 7\n",
    "scored_rows = []\n",
    "\n",
    "# Implement batching \n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        row.comments, \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits[:,target_emotion]\n",
    "        \n",
    "    logits = torch.exp(logits)\n",
    "    row_dict['score'] = torch.mean(logits)\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e7f2a88e-c4f5-4c87-bb2f-c3388a93b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(query_job.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7f053492-87a3-4bae-95e8-a0baf8826202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 ms, sys: 7.5 ms, total: 44.6 ms\n",
      "Wall time: 26.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = tokenizer(\n",
    "    rows[6]['comments'], \n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512, \n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e763c6ae-25b9-465c-a6fd-c4c41000c95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 465])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ca804954-aca4-4332-bf13-8eba36c3e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.4 s, sys: 10.8 s, total: 43.2 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:,target_emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0747d7e6-c5c9-49c6-9454-b5b1c2193f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.81 s, sys: 964 ms, total: 7.77 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for comment in rows[6]['comments']:\n",
    "    inputs = tokenizer(\n",
    "        comment, \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits[:,target_emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e362c430-a52b-4571-a279-32e89b140ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 s, sys: 10.6 s, total: 42.7 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = tokenizer(\n",
    "    rows[6]['comments'], \n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:,target_emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3fa5d923-3bc0-4adb-94da-6706d885ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 4.58 s, total: 22.7 s\n",
      "Wall time: 6.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import math\n",
    "all_logits = torch.empty((0))\n",
    "\n",
    "comments_list = rows[6]['comments']\n",
    "\n",
    "batch_size = 16\n",
    "comments_list_size = len(comments_list)\n",
    "batches = math.ceil(comments_list_size / batch_size)\n",
    "\n",
    "for i in range(batches):\n",
    "    comments = comments_list[i*batch_size:(i+1)*batch_size]\n",
    "    inputs = tokenizer(\n",
    "        comments, \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits[:,target_emotion]\n",
    "        all_logits = torch.cat((all_logits, logits), dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "304d0755-5bfa-4efa-b546-ac287f754e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 56s, sys: 2min 7s, total: 12min 3s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import math\n",
    "\n",
    "# Batching script\n",
    "\n",
    "rows = query_job.result() \n",
    "target_emotion = 7\n",
    "batch_size = 16\n",
    "scored_rows = []\n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    if len(row.comments) <= batch_size:\n",
    "        # print('below batch size')\n",
    "        inputs = tokenizer(\n",
    "            row.comments, \n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            all_logits = model(**inputs).logits[:,target_emotion]\n",
    "\n",
    "    else:\n",
    "        # print('batching activated')\n",
    "        all_logits = torch.empty((0))\n",
    "        comments_len = len(row.comments)\n",
    "        batches = math.ceil(comments_len / batch_size)\n",
    "        \n",
    "        for i in range(batches):\n",
    "            comments = row.comments[i*batch_size:(i+1)*batch_size]\n",
    "            # print(len(comments))\n",
    "            inputs = tokenizer(\n",
    "                comments, \n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # print(inputs.input_ids.shape)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits[:,target_emotion]\n",
    "                all_logits = torch.cat((all_logits, logits), dim=0)\n",
    "                # print(logits)\n",
    "\n",
    "    # print('logits:', all_logits)\n",
    "    scores = torch.exp(all_logits)\n",
    "    row_dict['score'] = torch.mean(scores)\n",
    "    # print('score:', row_dict['score'])\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d0416758-1fe3-4532-a455-db4f88eb9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rows_6 = sorted(scored_rows, key=lambda row: row['score'], reverse=True)\n",
    "\n",
    "for row in sorted_rows_6:\n",
    "    print(row['url'])\n",
    "    print(row['score'])\n",
    "    print('')\n",
    "\n",
    "# Output removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "86d672ba-7098-4bb4-a21b-db3b38a32a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 51s, sys: 28.6 s, total: 4min 20s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "rows = query_job.result() \n",
    "target_emotion = 7\n",
    "scored_rows = []\n",
    "\n",
    "# Test: roberta emotions - rank by average total curiosity - speedtest \n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    total = 0\n",
    "    for comment in row.comments:\n",
    "        inputs = tokenizer(comment, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            total += np.exp(logits[0][target_emotion])\n",
    "\n",
    "    row_dict['score'] = total / len(row.comments)\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "76cda5c2-9017-40da-b1a3-10e73093730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_rows_7 = sorted(scored_rows, key=lambda row: row['score'], reverse=True)\n",
    "\n",
    "for row in sorted_rows_7:\n",
    "    print(row['url'])\n",
    "    print(row['score'])\n",
    "    print('')\n",
    "\n",
    "# Output removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7ab3b6a4-a9ec-4e22-8686-64cb35dab7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like batching isn't effective unless MPS is active\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "81ff6b0e-fcae-4ce4-9a56-6d4e531b01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c09d7771-8cec-430e-8bf2-5be3ca7063ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2dc009cb-62d5-455b-8e58-937d11e291d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.5 s, sys: 6.61 s, total: 41.1 s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import math\n",
    "\n",
    "# Batching script\n",
    "\n",
    "rows = query_job.result() \n",
    "target_emotion = 7\n",
    "batch_size = 16\n",
    "scored_rows = []\n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    if len(row.comments) <= batch_size:\n",
    "        # print('below batch size')\n",
    "        inputs = tokenizer(\n",
    "            row.comments, \n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(mps_device)\n",
    "        with torch.no_grad():\n",
    "            all_logits = model(**inputs).logits[:,target_emotion]\n",
    "\n",
    "    else:\n",
    "        # print('batching activated')\n",
    "        all_logits = torch.empty((0)).to(mps_device)\n",
    "        comments_len = len(row.comments)\n",
    "        batches = math.ceil(comments_len / batch_size)\n",
    "        \n",
    "        for i in range(batches):\n",
    "            comments = row.comments[i*batch_size:(i+1)*batch_size]\n",
    "            # print(len(comments))\n",
    "            inputs = tokenizer(\n",
    "                comments, \n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(mps_device)\n",
    "            # print(inputs.input_ids.shape)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits[:,target_emotion]\n",
    "                all_logits = torch.cat((all_logits, logits), dim=0)\n",
    "                # print(logits)\n",
    "\n",
    "    # print('logits:', all_logits)\n",
    "    scores = torch.exp(all_logits)\n",
    "    row_dict['score'] = torch.mean(scores)\n",
    "    # print('score:', row_dict['score'])\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9fb7d49d-079a-43e8-b3b1-9097fe833d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 21s, sys: 25 s, total: 4min 46s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "rows = query_job.result() \n",
    "target_emotion = 7\n",
    "scored_rows = []\n",
    "\n",
    "# Test: roberta emotions - rank by average total curiosity - speedtest \n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    total = 0\n",
    "    for comment in row.comments:\n",
    "        inputs = tokenizer(comment, truncation=True, max_length=512, return_tensors=\"pt\").to(mps_device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            total += torch.exp(logits[0][target_emotion])\n",
    "\n",
    "    row_dict['score'] = total / len(row.comments)\n",
    "    scored_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8cd75-79f8-4c32-b8de-e569e28e0d35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Makes a lot more sense. Using plain CPU, the serial script ran in 2-3 minutes, while the batched ran in 3-4. With MPS enabled, batching is down to 1.5m while serial is 4+. \n",
    "\n",
    "---\n",
    "\n",
    "Collecting the useful code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb728af-d6ea-45d0-8ef5-7bf7fad92ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery load\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "QUERY = (\n",
    "    'SELECT * FROM `article-source.article_views.hackernews`')\n",
    "query_job = client.query(QUERY)  \n",
    "rows = query_job.result()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9fe5f-7137-422f-b7df-cbd2d7a72a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch and Transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c677212-f50a-4e17-8dcc-95a027848ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Tokenizer load\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01dded-7754-4b35-b643-624ef93ad5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label and ID mappings\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c04e6-66f0-4c6b-a32a-1d24befe1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching script\n",
    "import math\n",
    "\n",
    "\n",
    "target_emotion = 7\n",
    "batch_size = 16\n",
    "scored_rows = []\n",
    "\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['url'] = row.detail_url\n",
    "    row_dict['comments'] = row.comments\n",
    "\n",
    "    if len(row.comments) <= batch_size:\n",
    "        inputs = tokenizer(\n",
    "            row.comments, \n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            all_logits = model(**inputs).logits[:,target_emotion]\n",
    "\n",
    "    else:\n",
    "        all_logits = torch.empty((0))\n",
    "        comments_len = len(row.comments)\n",
    "        batches = math.ceil(comments_len / batch_size)\n",
    "        \n",
    "        for i in range(batches):\n",
    "            comments = row.comments[i*batch_size:(i+1)*batch_size]\n",
    "            inputs = tokenizer(\n",
    "                comments, \n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits[:,target_emotion]\n",
    "                all_logits = torch.cat((all_logits, logits), dim=0)\n",
    "\n",
    "    scores = torch.exp(all_logits)\n",
    "    row_dict['score'] = torch.mean(scores)\n",
    "    scored_rows.append(row_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
